{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc525ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import requests\n",
    "import itertools\n",
    "import json\n",
    "import time\n",
    "import sys\n",
    "excluded = [\"average\", \"called\", 'lifespan', 'maximum', 'minimum'] #words to completely ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47330896",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_web = spacy.load(\"en_core_web_lg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd622dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_sci = spacy.load(\"en_core_sci_lg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca195d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#add patterns.json (make sure it's saved in the same directory)\n",
    "\n",
    "ruler_web = nlp_web.add_pipe(\"entity_ruler\", first=True)\n",
    "ruler_sci = nlp_sci.add_pipe(\"entity_ruler\", first=True)\n",
    "\n",
    "with open('./patterns.json') as json_file:\n",
    "    patterns = json.load(json_file)\n",
    "    \n",
    "ruler_web.add_patterns(patterns)\n",
    "ruler_sci.add_patterns(patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e64c0e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#run this and then the block above to update patterns.json\n",
    "\n",
    "# nlp_web.remove_pipe(\"entity_ruler\")\n",
    "# nlp_sci.remove_pipe(\"entity_ruler\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99026c5f",
   "metadata": {},
   "source": [
    "# General functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e0e7e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "#returns the results of a query after making sure it worked\n",
    "#returnTries: whether the tries it took for the query to be successfull, should be returned\n",
    "def querySPARQL(query, returnTries=False):\n",
    "    url = 'https://query.wikidata.org/sparql'\n",
    "    results = \"\"\n",
    "    i = 1\n",
    "    while (str(results) != \"<Response [200]>\"):\n",
    "        results = requests.get(url, params={'query': query, 'format': 'json'})\n",
    "        if str(results) == \"<Response [200]>\":\n",
    "            results = results.json()\n",
    "            if returnTries: return results, i\n",
    "            return results\n",
    "        else:\n",
    "            time.sleep(.2)\n",
    "            i += 1\n",
    "\n",
    "#returns the wikidata id of a string\n",
    "#depth: index of the result to return\n",
    "#prop: whether it's a property instead of an entity\n",
    "#returnAll: whether all results should be returned in a list (depth will be ignored)\n",
    "def getWikidataEntityId(term, depth=0, prop=False, returnAll=False):\n",
    "    url = 'https://www.wikidata.org/w/api.php'\n",
    "    params = {'action':'wbsearchentities', 'language':'en', 'format':'json', 'search':str(term)}\n",
    "    if prop: params['type'] = 'property'\n",
    "    json = requests.get(url,params).json()\n",
    "    if 'search' in json:\n",
    "        if returnAll: #return a list of all results\n",
    "            ids = []\n",
    "            for item in json['search']: ids.append(item['id'])\n",
    "            return ids\n",
    "        elif len(json['search']) > depth: #return the nth result where n is depth\n",
    "            return json['search'][depth]['id']\n",
    "    return ''\n",
    "\n",
    "#tries to return the entity corresponding to a term with both parsers, and returns '' otherwise\n",
    "def termToEnt(term):\n",
    "    parse = nlp_sci(term)\n",
    "    if not parse.ents: parse = nlp_web(term)\n",
    "    if not parse.ents: return \"\"\n",
    "    return parse.ents[0]\n",
    "\n",
    "def removePlural(term):\n",
    "    if len(term) >= 4:\n",
    "        if term[-3] in [\"s\",\"x\",\"z\"]: return term[:-2]\n",
    "        if term[-4:-2] in [\"ss\",\"sh\",\"ch\"]: return term[:-2]\n",
    "        if term[-3:] == \"ies\": return term[:-3]+'y'\n",
    "    return term[:-1]\n",
    "\n",
    "#returns a list of entityIDs for a term\n",
    "#prop: whether it's a property instead of an entity\n",
    "def termToIds(term, prop):\n",
    "    if not term: return []\n",
    "    \n",
    "    IDS = []\n",
    "    ents = [termToEnt(term)]\n",
    "    \n",
    "    #use the initial term to get IDs\n",
    "    IDS.extend(getWikidataEntityId(term, returnAll=True, prop=prop))\n",
    "    \n",
    "    #remove 's' from the term in case it's plural, and add it and its entities\n",
    "    if term[-1] == 's':\n",
    "        ents.append(termToEnt(removePlural(term)))\n",
    "        IDS.extend(getWikidataEntityId(removePlural(term), returnAll=True, prop=prop))\n",
    "        \n",
    "    #add IDs of the entities in the term, found by the parser\n",
    "    for ent in ents:\n",
    "        if str(ent) != \"\":\n",
    "            #if the ID is stored in patterns.json, return only that ID\n",
    "            if ent.label_[0] in [\"P\",\"Q\"]:\n",
    "                return [ent.label_]\n",
    "\n",
    "            #add the IDs that this entity gives\n",
    "            IDS.extend(getWikidataEntityId(ent, returnAll=True, prop=prop))\n",
    "\n",
    "    #split the term into words and get more IDs from those\n",
    "    splitTerms = term.strip().split(\" \")\n",
    "    for splitTerm in splitTerms:\n",
    "        ents = [termToEnt(splitTerm)]\n",
    "        if splitTerm[-1] == 's': ents.append(termToEnt(removePlural(splitTerm)))\n",
    "        for ent in ents:\n",
    "            if str(ent) != \"\":\n",
    "                if ent.label_[0] in [\"P\",\"Q\"]:\n",
    "                    IDS.append(ent.label_)\n",
    "                IDS.extend(getWikidataEntityId(ent, returnAll=True, prop=prop))\n",
    "\n",
    "    #return the list without duplicates\n",
    "    return list(dict.fromkeys(IDS))\n",
    "\n",
    "#returns a list of skos:altLabels for an ID\n",
    "def getAltLabelsQuerySPARQL(ID):\n",
    "    query = '''SELECT ?name WHERE { wd:''' + ID + ' skos:altLabel ?name }'\n",
    "    altLabels = []\n",
    "    results = querySPARQL(query)\n",
    "    if results['head']:\n",
    "        for item in results['results']['bindings']:\n",
    "            altLabels.append(item['name']['value'].casefold())\n",
    "            #also add possible plurals\n",
    "            altLabels.append(item['name']['value'].casefold()+'s')\n",
    "\n",
    "    return altLabels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8336e256",
   "metadata": {},
   "source": [
    "# Functions for how and verb questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc2fc61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that returns an ID (or all IDs) for a given property or entity \n",
    "def getID(term, depth=0, property = False, returnAll = False):\n",
    "    url = 'https://www.wikidata.org/w/api.php'\n",
    "    params = {'action':'wbsearchentities', \n",
    "      'language':'en',\n",
    "      'format':'json'} \n",
    "    params['search'] = str(term)\n",
    "    if property: params['type'] = 'property'\n",
    "    json = requests.get(url, params).json()\n",
    "    ID = ''\n",
    "    if 'search' in json:\n",
    "        if returnAll: #return a list of all results\n",
    "            IDs = []\n",
    "            for item in json['search']:\n",
    "                IDs.append(item['id'])\n",
    "            return IDs\n",
    "        elif len(json['search']) > depth: #return nth result where n is depth\n",
    "            ID = json['search'][depth]['id']\n",
    "    return ID         \n",
    "\n",
    "def querySearch(entID, propID):\n",
    "    query = 'SELECT ?answerLabel WHERE { wd:' + entID + ' wdt:' + propID + ' ?answer . SERVICE wikibase:label {  bd:serviceParam wikibase:language \"en\" .  ?answer rdfs:label ?answerLabel . } }'\n",
    "    url = 'https://query.wikidata.org/sparql'\n",
    "\n",
    "    data = requests.get(url, params={'query': query, 'format': 'json'}).json()\n",
    "\n",
    "    if (data['results']['bindings'] == []):\n",
    "        return 0\n",
    "    else:\n",
    "        for item in data['results']['bindings']:\n",
    "            for var in item :\n",
    "                answers = []\n",
    "                answers.append(item[var]['value'])\n",
    "                return answers\n",
    "\n",
    "# Function to find entity of parsed questions\n",
    "def findEntity(parse):\n",
    "    entity = ''\n",
    "    numberOfNouns = 0\n",
    "    i = 0\n",
    "    for token in parse:\n",
    "        if token.pos_ == 'NOUN' or token.pos_ == 'PRON':\n",
    "            numberOfNouns += 1 # First checks the number of nouns in the question\n",
    "    if numberOfNouns == 1:\n",
    "        for token in parse: # The search criteria if only one noun is present\n",
    "            if token.dep_ == 'nsubj'or (token.dep_ == 'attr' and token.pos_ == 'NOUN'):\n",
    "                entity = token.lemma_\n",
    "                break\n",
    "            if token.dep_ == 'amod' or token.dep_ == 'compound':\n",
    "                if (token.head.dep_ == 'amod' or token.head.dep_ == 'compound'):\n",
    "                    entity = token.lemma_ + ' ' + token.head.lemma_ + ' ' + token.head.head.lemma_\n",
    "                else:\n",
    "                    entity = token.lemma_ + ' ' + token.head.lemma_\n",
    "                break\n",
    "            i += 1\n",
    "    else:\n",
    "        for token in parse: # Search criterias for when there are multiple nouns in question\n",
    "            if token.lemma_ not in excluded:\n",
    "                if ((token.dep_ == 'nsubj' and token.head.dep_ == 'ccomp') or token.pos_ == 'PART'):\n",
    "                    entity = token.head.lemma_\n",
    "                    break\n",
    "                elif token.pos_ == 'ADJ' and token.head.dep_ == 'nsubj' and token.head.head.pos_ != 'AUX':\n",
    "                    entity = token.lemma_ + ' ' + token.head.lemma_\n",
    "                    break\n",
    "                elif ((token.dep_ == 'amod' and token.head.dep_ == 'pobj') or (token.dep_ == 'compound' and token.head.dep_ == 'pobj') or \n",
    "                (token.dep_ == 'compound' and token.head.dep_ == 'compound') or (token.dep_ == 'compound') and token.head.dep_ == 'nsubj' and token.head.head.pos_ == 'VERB'):\n",
    "                    if (token.head.dep_ == 'amod' or token.head.dep_ == 'compound'):\n",
    "                        entity = token.lemma_ + ' ' + token.head.lemma_ + ' ' + token.head.head.lemma_\n",
    "                    else:\n",
    "                        entity = token.lemma_ + ' ' + token.head.lemma_   \n",
    "                    break\n",
    "                i += 1 \n",
    "        if entity == '': # Second search for when there are multiple nouns in the question\n",
    "            for token in parse:\n",
    "                if (token.head.head.lemma_ == 'of'):\n",
    "                    if token.dep_ == 'compound' or token.dep_ == 'amod':\n",
    "                        if (token.head.dep_ == 'amod' or token.head.dep_ == 'compound'):\n",
    "                            entity = token.lemma_ + ' ' + token.head.lemma_ + ' ' + token.head.head.lemma_\n",
    "                        else:\n",
    "                            entity = token.lemma_ + ' ' + token.head.lemma_  \n",
    "                    else:\n",
    "                        entity = token.head.lemma_\n",
    "                    break\n",
    "\n",
    "        if entity == '': # Final search for when there are multiple nouns in the question\n",
    "            for token in parse:\n",
    "                if ((token.dep_ == 'nsubj' and token.head.pos_ == 'AUX')):\n",
    "                    entity = token.lemma_ \n",
    "                    break   \n",
    "                elif token.dep_ == 'pobj':\n",
    "                    entity = token.lemma_\n",
    "                    break\n",
    "      \n",
    "    return entity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c1ed8ab",
   "metadata": {},
   "source": [
    "# Verb questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ac5349",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to perform query searches for 'verb' questions\n",
    "def verbQuerySPARQL(ent, prop):\n",
    "    query = 'SELECT ?answerLabel WHERE { wd:' + ent + ' wdt:' + prop + ''' ?answer .\n",
    "      SERVICE wikibase:label { bd:serviceParam wikibase:language \"en\" . }}'''\n",
    "    results = querySPARQL(query)\n",
    "    answers = []\n",
    "    if results['head']:\n",
    "        for item in results['results']['bindings']:\n",
    "            for var in item:\n",
    "                print(\"                          \", end=\"\\r\") #clear the 'loading' message\n",
    "                answers.append(item[var]['value'])\n",
    "    return answers\n",
    "\n",
    "\n",
    "# Main 'verb' questions function\n",
    "def verbQuestions(parse):\n",
    "    verb = ''\n",
    "    for token in parse:\n",
    "        if token.pos_  == 'VERB':\n",
    "            verb = token.lemma_\n",
    "            break\n",
    "            \n",
    "    entity, entID, propID = '', '', ''\n",
    "    for token in parse:\n",
    "        if (token.dep_ == 'nsubj' or token.dep_ == 'dobj') and (token.head.lemma_ == verb or token.head.pos_ == 'AUX') and token.pos_ == 'NOUN':\n",
    "            entity = token.lemma_ # First attempt at finding the entity\n",
    "        if token.pos_  == 'VERB':\n",
    "            ents = nlp_web(token.lemma_).ents\n",
    "            if ents:\n",
    "                if hasattr(ents[0], 'label_') and ents[0].label_[0] == \"P\":\n",
    "                    propID = ents[0].label_ # Search for the property ID in the patterns file\n",
    "\n",
    "    if not entity: entity = findEntity(parse) # If entity was not found prior, use the findEntity function defined above\n",
    "    entID = termToIds(entity, False)\n",
    "    answerFound = False\n",
    "    for entityID in entID:\n",
    "        answers = verbQuerySPARQL(entityID, propID)\n",
    "        if answers: return answers\n",
    "    return []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def0ef91",
   "metadata": {},
   "source": [
    "# How questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc838d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to perform query searches for 'how' questions\n",
    "def howQuerySPARQL(ent, prop):\n",
    "    query = 'SELECT ?answerLabel WHERE { wd:' + ent + ' wdt:' + prop + ''' ?answer .\n",
    "      SERVICE wikibase:label { bd:serviceParam wikibase:language \"en\" . }}'''\n",
    "    results = querySPARQL(query)\n",
    "    answers = []\n",
    "    if results['head']:\n",
    "        for item in results['results']['bindings']:\n",
    "            for var in item:\n",
    "                print(\"                          \", end=\"\\r\") #clear the 'loading' message\n",
    "                answers.append(item[var]['value'])\n",
    "    return answers\n",
    "\n",
    "# Checks the number of nouns in the question\n",
    "def alternateHowQuestion(parse):\n",
    "    numberOfNouns = 0\n",
    "    for token in parse:\n",
    "        if token.pos_ == 'NOUN': numberOfNouns += 1\n",
    "    return numberOfNouns > 1\n",
    "\n",
    "# Alternate method to find the property of the question\n",
    "def findAlternateProperty(parse):\n",
    "    prop = ''\n",
    "    amodPresent = False\n",
    "    i, j = 0, 0\n",
    "    for token in parse:\n",
    "        if token.lemma_ == 'many' and token.head.dep_ == 'nsubj':\n",
    "            amodPresent = True\n",
    "            prop = token.head.lemma_\n",
    "        if token.dep_ == 'amod' and token.head.dep_ == 'nsubj' and token.lemma_ != 'many':\n",
    "            j += 1\n",
    "        if j > 0 and token.dep_ == 'nsubj':\n",
    "            prop = parse[i - j : i + 1].text\n",
    "        elif (token.pos_ == 'NOUN' and token.dep_ == 'compound' and token.head.dep_ == 'nsubj'):\n",
    "            prop = token.text + ' ' + token.head.text\n",
    "            break\n",
    "        elif token.pos_== 'NOUN' and token.dep_ == 'nsubj' and amodPresent == False:\n",
    "            prop = token.text\n",
    "        i += 1\n",
    "    \n",
    "    return prop\n",
    "\n",
    "# Main 'how' question function\n",
    "def howQuestions(parse):\n",
    "    entity, entID, propID = '', [], []\n",
    "    entity = findEntity(parse) # Search for the entity using the findEntity method defined above\n",
    "    storeOld = False\n",
    "    for token in parse:\n",
    "        ents = nlp_web(token.lemma_).ents\n",
    "        if ents:\n",
    "            if hasattr(ents[0], 'label_') and ents[0].label_[0] == \"P\":\n",
    "                propID = [ents[0].label_] # As a first attempt, search for the property in the patterns file\n",
    "            if propID == ['P4214']:\n",
    "                storeOld = True # If the property refers to age, don't let any other saved term overwrite it\n",
    "\n",
    "    if storeOld: propID = ['P4214']\n",
    "\n",
    "    if (alternateHowQuestion(parse) and propID == []): # If property is still not found, and there is more than one noun, use the alternate property search method\n",
    "        prop = findAlternateProperty(parse)\n",
    "        propID = termToIds(prop, True)\n",
    "    if not entity:\n",
    "        for ent in parse.ents:\n",
    "            entID = termToIds(str(ent), False)\n",
    "            if entID and propID:\n",
    "                answers = howQuestionAnswerLoop(propID, entID) # Cross search every saved entity and property ID\n",
    "                if answers: return answers\n",
    "    else:\n",
    "        entID = termToIds(entity, False)\n",
    "        if entID and propID:\n",
    "            answers = howQuestionAnswerLoop(propID, entID)\n",
    "            if answers: return answers\n",
    "    return []\n",
    "    \n",
    "def howQuestionAnswerLoop(IDS_P, IDS_E):\n",
    "    #query loop, add 1 new P and 1 new E every time\n",
    "    depth = 0 #index of the new ID_E and ID_P\n",
    "    while depth < len(IDS_E) or depth < len(IDS_P):\n",
    "        #show the progress of the loop\n",
    "        print(\"\\rloading:\", depth, \"/\", max(len(IDS_E), len(IDS_P)), end=\"\\r\")\n",
    "        \n",
    "        #query with all IDS_E up to depth and IDS_P[depth]\n",
    "        if depth < len(IDS_P):\n",
    "            for ID_E in IDS_E[:depth]:\n",
    "                answers = howQuerySPARQL(ID_E, IDS_P[depth])\n",
    "                if answers:\n",
    "                    print(\"                          \", end=\"\\r\") #clear the 'loading' message\n",
    "                    return answers\n",
    "        \n",
    "        #query with IDS_E[depth] and all IDS_P\n",
    "        if depth < len(IDS_E):\n",
    "            for ID_P in IDS_P[:depth+1]:\n",
    "                answers = howQuerySPARQL(IDS_E[depth], ID_P)\n",
    "                if answers:\n",
    "                    print(\"                          \", end=\"\\r\")\n",
    "                    return answers\n",
    "            \n",
    "        depth += 1\n",
    "    \n",
    "    print(\"                          \", end=\"\\r\")\n",
    "    return []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c28ee7",
   "metadata": {},
   "source": [
    "# What is the X of Y questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a6e741c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#performs a SPARQL query for a question in the form: SELECT ?answerLabel WHERE wd:ID_Y wdt:ID_X ?answer,\n",
    "#returns all answers\n",
    "def whatQuerySPARQL(prop, ent):\n",
    "    query = 'SELECT ?answerLabel WHERE { wd:' + ent + ' wdt:' + prop + ''' ?answer .\n",
    "      SERVICE wikibase:label { bd:serviceParam wikibase:language \"en\" . }}'''\n",
    "    results = querySPARQL(query)\n",
    "    answers = []\n",
    "    if results['head']:\n",
    "        for item in results['results']['bindings']:\n",
    "            for var in item:\n",
    "                print(\"                          \", end=\"\\r\") #clear the 'loading' message\n",
    "                answers.append(item[var]['value'])\n",
    "    return answers\n",
    "\n",
    "#prints the answer(s) to a question of the form: What/Who was/is/were (the/a/an) X of (the/a/an) Y?\n",
    "#returns all answers\n",
    "def answer_what(parse, question):\n",
    "    #get the X and Y\n",
    "    ofSplit = 0\n",
    "    for i, word in enumerate(question.split()):\n",
    "        if word in [\"of\", \"for\"]: ofSplit = i-2\n",
    "    x, y = [], []\n",
    "    for i, word in enumerate(question.split()[2:]):\n",
    "        if word in [\"the\", \"an\", \"a\"]: continue\n",
    "        if i == ofSplit: continue #X is done, start adding to Y\n",
    "        if word not in excluded:\n",
    "            if i < ofSplit: x.append(word.strip(\"?\")) #add to X\n",
    "            else: y.append(word.strip(\"?\")) #add to Y \n",
    "                \n",
    "    x = \" \".join(x); y = \" \".join(y) \n",
    "\n",
    "    #get the entityIDs corresponding to x and y\n",
    "    IDS_X = termToIds(x, True); IDS_Y = termToIds(y, False);\n",
    "\n",
    "    #query loop, add 1 new X and 1 new Y every time\n",
    "    depth = 0 #index of the new ID_Y and ID_X\n",
    "    while depth < len(IDS_Y) or depth < len(IDS_X):\n",
    "        #show the progress of the loop\n",
    "        print(\"\\rloading:\", depth, \"/\", max(len(IDS_Y), len(IDS_X)), end=\"\\r\")\n",
    "        \n",
    "        #query with all IDS_Y up to depth and IDS_X[depth]\n",
    "        if depth < len(IDS_X):\n",
    "            for ID_Y in IDS_Y[:depth]:\n",
    "                answers = whatQuerySPARQL(IDS_X[depth], ID_Y)\n",
    "                if answers:\n",
    "                    print(\"                          \", end=\"\\r\") #clear the 'loading' message\n",
    "                    return answers\n",
    "        \n",
    "        #query with IDS_Y[depth] and all IDS_X\n",
    "        if depth < len(IDS_Y):\n",
    "            for ID_X in IDS_X[:depth+1]:\n",
    "                answers = whatQuerySPARQL(ID_X, IDS_Y[depth])\n",
    "                if answers:\n",
    "                    print(\"                          \", end=\"\\r\")\n",
    "                    return answers\n",
    "            \n",
    "        depth += 1\n",
    "    \n",
    "    print(\"                          \", end=\"\\r\")\n",
    "    return []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4081e99",
   "metadata": {},
   "source": [
    "# Or questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab580ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#returns the answers to a question of the form: Is/Was/Are/Were X (a/an) Y(, Z) or W?\n",
    "def answer_isXaYor(parse, question):\n",
    "    #Dependency analysis\n",
    "    subject, attributes = [], []\n",
    "    for w in parse:\n",
    "        if w.dep_ == \"nsubj\":\n",
    "            for d in w.subtree:\n",
    "                if d.dep_ not in [\"det\", \"conj\", \"cc\", \"attr\", \"appos\"] and d.text not in excluded:\n",
    "                    subject.append(d.text.strip(', '))\n",
    "        if w.dep_ in [\"appos\", \"conj\", \"attr\"]:\n",
    "            currAttribute = [w.text]\n",
    "            for d in w.subtree:\n",
    "                if d.dep_ not in [\"det\", \"conj\", \"cc\", \"attr\", \"appos\"] and d.text not in excluded:\n",
    "                    currAttribute.append(d.text.strip(', '))\n",
    "            attributes.append(currAttribute)\n",
    "                    \n",
    "    subj = \" \".join(subject)\n",
    "        \n",
    "    #for all possible attributes / entities to compare to, use the function answer_isXaY\n",
    "    #and print that attribute if it's a superclass\n",
    "    answers = []\n",
    "    for attribute in attributes:\n",
    "        attr = \" \".join(attribute) \n",
    "        if answer_isXaY(parse, question, subj=subj, attr=attr):\n",
    "            answers.append(attr)\n",
    "    return answers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c2d788",
   "metadata": {},
   "source": [
    "# Are X and Y the same / different questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240458cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#returns the answer to a question that asks whether X and Y are different, or whether X and Y are the same\n",
    "#same: whether the question is whether X and Y are the same\n",
    "def answer_isXYdiff(parse, question, same=False):\n",
    "    #Dependency analysis\n",
    "    subject, attribute = [], []\n",
    "        \n",
    "    #question form 1\n",
    "    if \"different from\" in question or \"same as\" in question:\n",
    "        for w in parse:\n",
    "            if w.dep_ in [\"nsubj\", \"npadvmod\"]:\n",
    "                for d in w.subtree:\n",
    "                    if d.dep_ != \"det\" and d.text not in excluded and d.text != \"same\":\n",
    "                        subject.append(d.text)\n",
    "            if w.dep_ == \"pobj\":\n",
    "                for d in w.subtree:\n",
    "                    if d.dep_ != \"det\" and d.text not in excluded and d.text != \"same\":\n",
    "                        attribute.append(d.text)\n",
    "                        \n",
    "    #question form 2\n",
    "    else:\n",
    "        for w in parse:\n",
    "            if w.dep_ in [\"nsubj\",\"attr\"]:\n",
    "                for d in w.subtree:\n",
    "                    if d.dep_ not in [\"det\", \"conj\", \"cc\"] and d.text not in excluded and d.text not in [\"same\", \"thing\"]:\n",
    "                        subject.append(d.text)\n",
    "                    elif d.dep_ == \"conj\":\n",
    "                        for e in d.subtree:\n",
    "                            if e.dep_ != \"det\" and e.text not in excluded and e.text not in [\"same\", \"thing\"]:\n",
    "                                attribute.append(e.text) \n",
    "           \n",
    "    x = \" \".join(subject); y = \" \".join(attribute)\n",
    "    \n",
    "    #get the entityIDs corresponding to the subject and attribute\n",
    "    IDS_X = termToIds(x, False); IDS_Y = termToIds(y, False);\n",
    "    \n",
    "    #query loop 1, first IDS_X and then IDS_Y\n",
    "    for i, currIDS in enumerate([IDS_X, IDS_Y]):\n",
    "        for ID in currIDS:\n",
    "            otherList = [IDS_Y, IDS_X][i]\n",
    "            \n",
    "            #query whether one ID is in the 'different from' or 'said to be the same as' property of the other\n",
    "            queries = ['SELECT ?answer WHERE { wd:' + ID + ' wdt:P1889 ?answer . }',\n",
    "                       'SELECT ?answer WHERE { wd:' + ID + ' wdt:P460 ?answer . }']\n",
    "            for j, query in enumerate(queries):\n",
    "                results = querySPARQL(query)\n",
    "                for item in results['results']['bindings']:\n",
    "                    for var in item:\n",
    "                        if item[var]['value'].split('/')[-1] in otherList: #it is\n",
    "                            return([True,False][(int(same)+j)%2])\n",
    "                        \n",
    "    #query loop 2, first IDS_X and then IDS_Y   \n",
    "    for i, currIDS in enumerate([IDS_X, IDS_Y]):\n",
    "        for ID in currIDS:\n",
    "            #query whether the altLabels of one ID contains the other term\n",
    "            alts = getAltLabelsQuerySPARQL(ID)\n",
    "            if not set(alts).isdisjoint([y,x][i].split()) or \" \".join([y,x][i]) in alts: #it does\n",
    "                return([False,True][int(same)])\n",
    "            \n",
    "    #nothing was found, so X and Y are likely different\n",
    "    return([True,False][int(same)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b53d56f",
   "metadata": {},
   "source": [
    "# Yes/no questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe2ee486",
   "metadata": {},
   "outputs": [],
   "source": [
    "#returns the answer to a question that asks whether X has a property with value Y\n",
    "#same: whether the question is whether X and Y are the same\n",
    "def answer_isXpY(parse, question, subj, val):\n",
    "    #get entityIDs corresponding to the subject and object\n",
    "    IDS_X = termToIds(subj, False); IDS_Y = termToIds(val, False);\n",
    "        \n",
    "    for ID_X in IDS_X:\n",
    "        query = '''SELECT ?answer \n",
    "              WHERE { wd:''' + ID_X + ' ?prop ?answer . }'\n",
    "        results = querySPARQL(query)\n",
    "        for item in results['results']['bindings']:\n",
    "            for var in item:\n",
    "                if item[var]['value'].split('/')[-1] in IDS_Y:\n",
    "                    return True\n",
    "    return False\n",
    "    \n",
    "#returns the answer to a question of the form: Is/Was/Are/Were/Does/Did/Do/Can/Could X (a/an) Y?\n",
    "#Y can be an entity or a property\n",
    "def answer_isXY(parse, question):\n",
    "    #Dependency analysis\n",
    "    subject, values = [], []\n",
    "    for w in parse:\n",
    "        if w.dep_ == \"nsubj\":\n",
    "            for d in w.subtree:\n",
    "                if d.dep_ != \"det\" and d.text not in excluded:\n",
    "                    subject.append(d.text.strip(', '))\n",
    "    for w in parse:\n",
    "        if w.text not in subject:\n",
    "            currValue = []\n",
    "            for d in w.subtree:\n",
    "                if d.dep_ not in [\"det\", \"nsubj\", \"punct\", \"ROOT\"] and d.text not in excluded:\n",
    "                    currValue.append(d.text.strip(', '))\n",
    "            if currValue: values.append(\" \".join(currValue))\n",
    "                    \n",
    "    subj = \" \".join(subject)\n",
    "    \n",
    "    #try to find Y in a property of X with the function answer_isXpY\n",
    "    for i, value in enumerate(values):\n",
    "        print(\"\\rloading:\", i, \"/\", len(values), end=\"\\r\")\n",
    "            \n",
    "        if answer_isXpY(parse, question, subj, value):\n",
    "            print(\"                          \", end=\"\\r\")\n",
    "            return True\n",
    "        \n",
    "    print(\"                          \", end=\"\\r\")\n",
    "        \n",
    "    #use the function answer_isXaY in case Y is an entity\n",
    "    if parse[0].text.casefold() in [\"is\",\"was\",\"are\",\"were\"]:\n",
    "        return(answer_isXaY(parse, question))\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c53ec1d0",
   "metadata": {},
   "source": [
    "# Is X a Y questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad43612d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#recursively checks superclasses of a class(cls) to find a not necessarily direct superclass(sprCls)\n",
    "#depth: the number of superclasses away from the original class\n",
    "def isSuperclass(cls, sprClasses, sprTerm, checked=set(), depth=0):\n",
    "    url = 'https://query.wikidata.org/sparql'\n",
    "    \n",
    "    #query all superclasses (and 'instance of' classes) of this class\n",
    "    if depth < 3:\n",
    "        queries = ['SELECT ?answer WHERE { wd:' + cls + ' wdt:P279 ?answer . }',\n",
    "                   'SELECT ?answer WHERE { wd:' + cls + ' wdt:P31 ?answer . }']\n",
    "        for query in queries:\n",
    "            results, tries = querySPARQL(query, returnTries=True)\n",
    "            if results['head']:\n",
    "                for i, item in enumerate(results['results']['bindings']):\n",
    "                    for var in item:\n",
    "                        actualSpr = item[var]['value'].split('/')[-1]\n",
    "\n",
    "                        if actualSpr not in checked: #don't do the same queries again\n",
    "                            checked.add(actualSpr)\n",
    "                            \n",
    "                            #check whether the actual superclass of this class is the correct one\n",
    "                            if actualSpr in sprClasses: return True, checked\n",
    "                            altLabels = getAltLabelsQuerySPARQL(actualSpr)\n",
    "                            if sprTerm.casefold() in altLabels: return True, checked\n",
    "                            \n",
    "                            found, newChecked = isSuperclass(actualSpr, sprClasses, sprTerm, checked=checked, depth=depth+1)\n",
    "                            checked.update(newChecked)\n",
    "                            if found: return True, checked\n",
    "            \n",
    "    return False, checked\n",
    "    \n",
    "#returns the answer to a question of the form: Is/Was/Are/Were X (a/an) Y?\n",
    "#subj: subject given by the function answer_isXaYor\n",
    "#attr: attribute given by the function answer_isXaYor\n",
    "def answer_isXaY(parse, question, subj=\"\", attr=\"\"):\n",
    "    if not subj:\n",
    "        #Dependency analysis\n",
    "        subject, attribute = [], []\n",
    "        for w in parse:\n",
    "            if w.dep_ == \"nsubj\":\n",
    "                for d in w.subtree:\n",
    "                    if d.dep_ != \"det\" and d.text not in excluded: subject.append(d.text)\n",
    "            elif w.dep_ == \"attr\":\n",
    "                for d in w.subtree:\n",
    "                    if d.dep_ != \"det\" and d.text not in excluded:\n",
    "                        attribute.append(d.text)\n",
    "                        \n",
    "        subj = \" \".join(subject); attr = \" \".join(attribute) \n",
    "                    \n",
    "    #get the entityIDs corresponding to subj and attr\n",
    "    IDS_S = termToIds(subj, False); IDS_O = termToIds(attr, False);\n",
    "    \n",
    "    #query loop\n",
    "    for i, ID_S in enumerate(IDS_S):\n",
    "        print(\"\\rloading:\", i, \"/\", len(IDS_S), end=\"\\r\")\n",
    "        \n",
    "        #use the isSuperclass function to recursively check whether an ID_O is a(n inderect) superclass of ID_S\n",
    "        found, temp = isSuperclass(ID_S, IDS_O, attr)\n",
    "        if found:\n",
    "            print(\"                          \", end=\"\\r\")\n",
    "            return True  \n",
    "        \n",
    "    print(\"                          \", end=\"\\r\")\n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4af9527",
   "metadata": {},
   "source": [
    "# List questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e26e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "#returns the answers to a question of the form: List/Name X of/for/in Y\n",
    "def answer_listXofY(parse, question, reverse=False):\n",
    "    #Dependency analysis\n",
    "    ent, prop = [], []\n",
    "    for w in parse:\n",
    "        if w.dep_ in [\"nsubj\"]:\n",
    "            for d in w.subtree:\n",
    "                if d.dep_ not in [\"det\", \"prep\", \"dobj\", \"pobj\"] and d.text not in excluded:\n",
    "                    ent.append(d.text)\n",
    "        if w.dep_ in [\"relcl\", \"dobj\", \"amod\", \"pobj\", \"prep\", \"attr\"]:\n",
    "            for d in w.subtree:\n",
    "                if d.dep_ not in [\"det\", \"acl\", \"nsubj\"] and d.text not in excluded:\n",
    "                    prop.append(d.text)\n",
    "\n",
    "    ent = \" \".join(ent); prop = \" \".join(prop) \n",
    "                    \n",
    "    if reverse: temp = ent; ent = prop; prop = temp;\n",
    "        \n",
    "    #get the entityIDs corresponding to x and y\n",
    "    IDS_X = termToIds(prop, True); IDS_Y = termToIds(ent, False);\n",
    "\n",
    "    #query loop, add 1 new X and 1 new Y every time\n",
    "    depth = 0 #index of the new ID_Y and ID_X\n",
    "    while depth < len(IDS_Y) or depth < len(IDS_X):\n",
    "        #show the progress of the loop\n",
    "        print(\"\\rloading:\", depth, \"/\", max(len(IDS_Y), len(IDS_X)), end=\"\\r\")\n",
    "        \n",
    "        #query with all IDS_Y up to depth and IDS_X[depth]\n",
    "        if depth < len(IDS_X):\n",
    "            for ID_Y in IDS_Y[:depth]:\n",
    "                answers = whatQuerySPARQL(IDS_X[depth], ID_Y)\n",
    "                if answers:\n",
    "                    print(\"                          \", end=\"\\r\") #clear the 'loading' message\n",
    "                    return answers\n",
    "        \n",
    "        #query with IDS_Y[depth] and all IDS_X\n",
    "        if depth < len(IDS_Y):\n",
    "            for ID_X in IDS_X[:depth+1]:\n",
    "                answers = whatQuerySPARQL(ID_X, IDS_Y[depth])\n",
    "                if answers:\n",
    "                    print(\"                          \", end=\"\\r\")\n",
    "                    return answers\n",
    "            \n",
    "        depth += 1\n",
    "    \n",
    "    print(\"                          \", end=\"\\r\")\n",
    "    if not reverse and (ent or prop): return answer_listXofY(parse, question, reverse=True)\n",
    "    return []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05adea4c",
   "metadata": {},
   "source": [
    "# Count Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bcd71e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to perform query searches for count questions\n",
    "def queryCountSearch(ent, prop):\n",
    "    query = 'SELECT (COUNT(?prop) as ?answer) WHERE { wd:' + ent + ' wdt:' + prop + ' ?prop . SERVICE wikibase:label {  bd:serviceParam wikibase:language \"en\" . } }'\n",
    "    results = querySPARQL(query)\n",
    "    answers = []\n",
    "    if results['head']:\n",
    "        for item in results['results']['bindings']:\n",
    "            for var in item:\n",
    "                answers.append(item[var]['value'])\n",
    "                print(\"                          \", end=\"\\r\") #clear the 'loading' message\n",
    "    return answers\n",
    "\n",
    "# Function to check how many nouns is in a question \n",
    "def moreThanOneNoun(parse):\n",
    "    numberOfNouns = 0\n",
    "    for token in parse:\n",
    "        if token.pos_ == 'NOUN' or token.pos_ == 'PROPN':\n",
    "            numberOfNouns += 1\n",
    "    return numberOfNouns > 1\n",
    "\n",
    "# Searching for the entity in count questions\n",
    "def findCountEntity(parse):\n",
    "    entity = []\n",
    "    doesPresent = False\n",
    "    for token in parse:\n",
    "        if entity == []:\n",
    "            if token.lemma_ == 'do':\n",
    "                doesPresent = True\n",
    "            if doesPresent:\n",
    "                if token.pos_ == 'PROPN':\n",
    "                    if (token.head.pos_ == 'PROPN'):\n",
    "                        entity = token.lemma_ + ' ' + token.head.lemma_\n",
    "                        break\n",
    "                    else:\n",
    "                        entity = token.lemma_\n",
    "                        break\n",
    "                if token.pos_ == 'NOUN':\n",
    "                    entity = token.lemma_\n",
    "                    break \n",
    "    if entity == []: # If entity was not found after first attempt, try second set of search criteria\n",
    "        for token in parse:\n",
    "            if token.pos_ == 'PROPN':\n",
    "                if (token.head.pos_ == 'PROPN'):\n",
    "                    entity = token.lemma_ + ' ' + token.head.lemma_\n",
    "                else:\n",
    "                    entity = token.lemma_\n",
    "                break\n",
    "            if not moreThanOneNoun(parse) and token.pos_ == 'NOUN':\n",
    "                entity = token.lemma_\n",
    "                break\n",
    "    if entity == []: # If entity is still not found, use the findEntity method defined above\n",
    "        entity = findEntity(parse)\n",
    "    return entity\n",
    "\n",
    "# Function to search for the property of count questions\n",
    "# It searches for tokens whose head is a noun, and the head's head token is not a verb\n",
    "def searchForCountProperty(parse):\n",
    "    prop, propID = [], []\n",
    "    for token in parse:\n",
    "        if token.lemma_ == 'many':\n",
    "            if token.head.pos_ == 'NOUN' and token.head.head.pos_ != 'VERB':\n",
    "                prop = token.head.lemma_\n",
    "                propID = getID(prop, property = True)\n",
    "    return propID\n",
    "\n",
    "# Performs two types of property checks\n",
    "def getCountProperty(parse):\n",
    "    propID = []\n",
    "    ents = []\n",
    "    for token in parse: # First, we search for the property in the patterns file\n",
    "        if token.pos_ == 'NUM':\n",
    "            propID = 'P1082'\n",
    "            break\n",
    "        ents = nlp_web(token.lemma_).ents\n",
    "        if ents:\n",
    "            if hasattr(ents[0], 'label_') and ents[0].label_[0] == \"P\":\n",
    "                propID = ents[0].label_\n",
    "\n",
    "    if propID == []: # If not found, we use the aforementioned searchForCountProperty function\n",
    "        propID = searchForCountProperty(parse)\n",
    "    return propID\n",
    "\n",
    "# Main count question function\n",
    "def countQuestion(parse):\n",
    "    entity, entID, propID = '', '', ''\n",
    "    entity = findCountEntity(parse) # Search for entity using specialised method\n",
    "    propID = getCountProperty(parse) # Obtain the property using the previously mentioned function\n",
    "    alt = False\n",
    "    if propID == 'P7725' or propID == 'P3395' or  propID == 'P1082': # If the property ID matches one of these, it means we use a query search that is similar to other question types\n",
    "        alt = True\n",
    "    answerFound = False\n",
    "    if entity == []: # If entity was not found using the function, search through the entities in parse.ents\n",
    "        for ent in parse.ents:\n",
    "            entID = termToIds(ent, False)\n",
    "            for entityID in entID:\n",
    "                if(alt):\n",
    "                    answers = howQuerySPARQL(entityID, propID) # We use the query search that is used for 'how' questions when alt is set true\n",
    "                    if answers: return answers\n",
    "                else:\n",
    "                    answers = countQuestionAnswerLoop(propID, entityID) # Otherwise, we use the standard count query search\n",
    "                    if answers: return answers  \n",
    "    else:\n",
    "        entID = termToIds(entity, False)\n",
    "        for entityID in entID:\n",
    "            if(alt):\n",
    "                answers = howQuerySPARQL(entityID, propID)\n",
    "                if answers: return answers\n",
    "            else:\n",
    "                answers = countQuestionAnswerLoop(propID, entityID)\n",
    "                if answers: return answers \n",
    "    return []\n",
    "\n",
    "def countQuestionAnswerLoop(IDS_P, IDS_E):\n",
    "    #query loop, add 1 new P and 1 new E every time\n",
    "    depth = 0 #index of the new ID_E and ID_P\n",
    "    while depth < len(IDS_E) or depth < len(IDS_P):\n",
    "        #show the progress of the loop\n",
    "        print(\"\\rloading:\", depth, \"/\", max(len(IDS_E), len(IDS_P)), end=\"\\r\")\n",
    "        \n",
    "        #query with all IDS_E up to depth and IDS_P[depth]\n",
    "        if depth < len(IDS_P):\n",
    "            for ID_E in IDS_E[:depth]:\n",
    "                answers = queryCountSearch(ID_E, IDS_P[depth])\n",
    "                if answers:\n",
    "                    print(\"                          \", end=\"\\r\") #clear the 'loading' message\n",
    "                    return answers\n",
    "        \n",
    "        #query with IDS_E[depth] and all IDS_P\n",
    "        if depth < len(IDS_E):\n",
    "            for ID_P in IDS_P[:depth+1]:\n",
    "                answers = queryCountSearch(IDS_E[depth], ID_P)\n",
    "                if answers:\n",
    "                    print(\"                          \", end=\"\\r\")\n",
    "                    return answers\n",
    "            \n",
    "        depth += 1\n",
    "    \n",
    "    print(\"                          \", end=\"\\r\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b70fb234",
   "metadata": {},
   "source": [
    "# Different language question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df1e9459",
   "metadata": {},
   "outputs": [],
   "source": [
    "def otherLanguage(qtype, language, parse):\n",
    "    with open('./language_codes.json') as json_file:\n",
    "        languageCodes = json.load(json_file)\n",
    "    languageLabel = \"\"\n",
    "    \n",
    "    for i in languageCodes:\n",
    "        file_languages = i[\"pattern\"].lower().split(\"; \")\n",
    "        for file_lang in file_languages:\n",
    "            if file_lang == language.lower():\n",
    "                languageLabel = i[\"label\"]\n",
    "    searches = {}                 \n",
    "    if qtype == 1: # What is the X word/name for Y?\n",
    "        i = 0\n",
    "        for word in parse : # iterate over the token objects\n",
    "            while (word.text == 'for'):  \n",
    "                j = i + 1\n",
    "                break\n",
    "            i += 1\n",
    "        searches[\"entity\"] = parse[j:i-1].text #entity\n",
    "        entID = termToIds(searches[\"entity\"], False)\n",
    "    elif qtype == 2: # What is Y called in X?\n",
    "        i = 0\n",
    "        for word in parse : # iterate over the token objects\n",
    "            while (word.lemma_ == 'be'):  \n",
    "                j = i + 1\n",
    "                break\n",
    "            while (word.lemma_ == \"call\"):\n",
    "                k = i\n",
    "                break\n",
    "            i += 1\n",
    "        searches[\"entity\"] = parse[j:k].text #entity\n",
    "        entID = termToIds(searches[\"entity\"], False)\n",
    "    elif qtype == 3: # How do I say Y in X? (Where X is a language)\n",
    "        i = 0\n",
    "        for word in parse : # iterate over the token objects\n",
    "            while (word.lemma_ == 'say'):  \n",
    "                j = i + 1\n",
    "                break\n",
    "            while (word.lemma_ == \"in\"):\n",
    "                k = i\n",
    "                break\n",
    "            i += 1\n",
    "        searches[\"entity\"] = parse[j:k].text #entity\n",
    "        entID = termToIds(searches[\"entity\"], False) \n",
    "    elif qtype == 4: # Other  \n",
    "        entity = []\n",
    "        for w in parse:\n",
    "            if w.dep_ in [\"pobj\"]:\n",
    "                for d in w.subtree:\n",
    "                    if d.dep_ not in [\"det\"] and d.text not in excluded:\n",
    "                        entity.append(d.text)\n",
    "        entID = termToIds(\" \".join(entity), False)\n",
    "        \n",
    "    for ID_E in entID:\n",
    "        answers = languageQuery(ID_E, languageLabel)\n",
    "        if answers: return answers\n",
    "    \n",
    "    return []\n",
    "\n",
    "def languageQuery(entID, languageLabel):\n",
    "    query = 'SELECT ?name WHERE { wd:'+entID+' rdfs:label ?name FILTER(lang(?name) = \"'+languageLabel+'\")}'\n",
    "    results = querySPARQL(query)\n",
    "    for item in results['results']['bindings']:\n",
    "        for var in item:\n",
    "            answers = [item[var]['value']]\n",
    "            if answers: return answers\n",
    "        if len(item) == 0:\n",
    "            return []\n",
    "    return []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f07cea4",
   "metadata": {},
   "source": [
    "# Description question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2aa6bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "def descriptionQuestion(parse):\n",
    "    entity = \"\"\n",
    "    if (parse[2].lemma_ == \"a\") or (parse[2].lemma_ == \"an\"): # What is a/an X?\n",
    "        i = 3\n",
    "    else: \n",
    "        i = 2 # What is X?\n",
    "    for word in parse[i:len(parse) - 1]:\n",
    "        entity = entity + \" \" + word.text\n",
    "    entID = termToIds(entity, False)\n",
    "        \n",
    "    for ID_E in entID:\n",
    "        answers = descriptionQuery(ID_E)\n",
    "        if answers: return answers\n",
    "        \n",
    "    return []\n",
    "        \n",
    "def descriptionQuery(entID):\n",
    "    query = '''SELECT ?description\n",
    "  WHERE {\n",
    "    wd:'''+entID+''' schema:description ?description .\n",
    "    FILTER(lang(?description) = \"en\")\n",
    "        }'''\n",
    "    results = querySPARQL(query)\n",
    "    for item in results['results']['bindings']:\n",
    "        for var in item:\n",
    "            answers = [item[var]['value']]\n",
    "            if answers: return answers\n",
    "        if len(item) == 0:\n",
    "            return []\n",
    "    return []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f44674a1",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08581a1b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def printAnswers(answers, question, depth=0, isListQuestion=False):\n",
    "    if answers: print(\", \".join(answers))\n",
    "    elif depth == 0 and not isListQuestion:\n",
    "        parse = nlp_web(question)\n",
    "        answers = answer_listXofY(parse, question)\n",
    "        printAnswers(answers, question, depth=1)\n",
    "    else:\n",
    "        print(\"null\")\n",
    "\n",
    "def main():\n",
    "    with open('./all_submitted_questions.json') as json_file:\n",
    "        questions = json.load(json_file)\n",
    "\n",
    "    for i, item in enumerate(questions):\n",
    "        if not(0 <= i < 999): continue\n",
    "        question = item['string']\n",
    "        print(i, question)\n",
    "        parse = nlp_web(question)\n",
    "        question_split = question.split()\n",
    "        isLanguageQuestion = False\n",
    "        \n",
    "        # What is the X word/name for Y?(Where X is a language) \n",
    "        if (len(parse) > 6) and (parse[0].lemma_ == \"what\") and (parse[4].lemma_ == \"word\" or parse[4].lemma_ == \"name\") and (parse[5].lemma_ == \"for\"):\n",
    "            qtype = 1\n",
    "            printAnswers(otherLanguage(qtype, parse[3].lemma_, parse), question)\n",
    "            isLanguageQuestion = True\n",
    "            \n",
    "         # What is Y called in X?(Where X is a language) \n",
    "        elif (len(parse) > 5) and (parse[0].lemma_ == \"what\") and (parse[3].lemma_ == \"call\") and (parse[4].lemma_ == \"in\"):\n",
    "            qtype = 2\n",
    "            printAnswers(otherLanguage(qtype, parse[len(parse) - 2].lemma_, parse), question)\n",
    "            isLanguageQuestion = True\n",
    "            \n",
    "        # How do I say Y in X? (Where X is a language) \n",
    "        elif (len(parse) > 5) and (parse[0].lemma_ == \"how\") and (parse[3].lemma_ == \"say\"):\n",
    "            qtype = 3\n",
    "            printAnswers(otherLanguage(qtype, parse[len(parse) - 2].lemma_, parse), question)\n",
    "            isLanguageQuestion = True\n",
    "            \n",
    "        # Other question with a language in it \n",
    "        elif True:\n",
    "            with open('./language_codes.json') as json_file:\n",
    "                languageCodes = json.load(json_file)\n",
    "            \n",
    "            breakLanguage = False\n",
    "            for word in parse:\n",
    "                if word.dep_ != \"amod\":\n",
    "                    for i in languageCodes:\n",
    "                        if i[\"pattern\"].lower() == word.lemma_.lower():\n",
    "                            language = i[\"label\"]\n",
    "                            breakLanguage = True\n",
    "                            break\n",
    "                    if breakLanguage:\n",
    "                        qtype = 4\n",
    "                        printAnswers(otherLanguage(qtype, word.lemma_, parse), question)\n",
    "                        isLanguageQuestion = True\n",
    "                        break\n",
    "                    \n",
    "        if isLanguageQuestion: continue\n",
    "            \n",
    "        #What is the X of Y question\n",
    "        if question_split[0].casefold() in [\"what\",\"who\",\"which\"] and question_split[1].casefold() in [\"was\",\"is\",\"were\",\"are\"] and (\"of\" in question_split or \"for\" in question_split) and (parse[4].lemma_ != \"word\" and parse[4].lemma_ != \"name\"):\n",
    "            printAnswers(answer_what(parse, question), question)\n",
    "          \n",
    "        #Yes/no questions\n",
    "        elif question_split[0].casefold() in [\"is\",\"was\",\"are\",\"were\",\"does\",\"did\",\"do\",\"can\",\"could\"]:\n",
    "            if \"different\" in question_split:\n",
    "                print([\"No\",\"Yes\"][answer_isXYdiff(parse, question)])\n",
    "\n",
    "            elif \"same\" in question_split:\n",
    "                print([\"No\",\"Yes\"][answer_isXYdiff(parse, question, same=True)])\n",
    "\n",
    "            elif \"or\" in question_split:\n",
    "                printAnswers(answer_isXaYor(parse, question), question)\n",
    "\n",
    "            else: print([\"No\",\"Yes\"][answer_isXY(parse, question)])\n",
    "            \n",
    "        elif question_split[0].casefold() == \"how\" and question_split[1].casefold() != \"many\":\n",
    "            answers = howQuestions(parse)\n",
    "            if not answers:\n",
    "                parse = nlp_sci(question)\n",
    "                answers = howQuestions(parse)\n",
    "            if not answers:\n",
    "                answers = answer_what(parse, \"What \" + question.split(' ', 2)[2])\n",
    "            printAnswers(answers, question)\n",
    "\n",
    "        elif question_split[0].casefold() == \"how\" and question_split[1].casefold() == \"many\":\n",
    "            answers = countQuestion(parse)\n",
    "            if not answers:\n",
    "                parse = nlp_sci(question)\n",
    "                answers = countQuestion(parse)\n",
    "            printAnswers(answers, question)\n",
    "                \n",
    "        #Other questions\n",
    "        else:\n",
    "            isListQuestion = False\n",
    "            #Try verb question\n",
    "            answers = verbQuestions(nlp_web(question))\n",
    "            if not answers:\n",
    "                #Try general/list question\n",
    "                answers = answer_listXofY(parse, question)\n",
    "                isListQuestion = True\n",
    "                if not answers:\n",
    "                    isListQuestion = False\n",
    "                    #What is/are (a/an) X? (description question)\n",
    "                    if (len(parse) > 2) and (parse[0].lemma_ == \"what\") and (parse[1].lemma_ == \"be\"):\n",
    "                        answers = descriptionQuestion(parse)\n",
    "            \n",
    "            printAnswers(answers, question, isListQuestion=isListQuestion)\n",
    "                    \n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2fae2c3511c076021ac74b34829ab840d3d593f31233c8a97891c26ce14fda1a"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
